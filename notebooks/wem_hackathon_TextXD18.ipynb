{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> TextXD 2018 Hack session<br><br>Word embedding models for charter schools:<br>Detecting discursive themes through querying neural nets\n",
    "<p style=\"text-align: center;\">Creator: Jaren Haber, PhD Candidate<br/>Institution: Department of Sociology, University of California, Berkeley<br/>December 2018\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "What are the major themes deployed in the website self-descriptions of charter schools--an educational innovation (7,000 schools strong!) intended to meet the particular educational desires of diverse communities?\n",
    "\n",
    "This notebook gives starter code to answer this question by loading a 300-dimension word embedding model (WEM),  iteratively querying it to detect coherent clusters of terms, and visualizing these clusters in two dimensions. The data come from my web-scraping of all 6,862 charter schools open in the 2015-16 school year. The embeddings were created in the word2vec implementation in gensim. \n",
    "\n",
    "This notebook was prepared for a hack session at [TextXD 2018](http://www.textxd.org/) at the [Berkeley Institute for Data Science (BIDS)](https://bids.berkeley.edu/), UC Berkeley. This code is available in [my GitHub repository](https://github.com/jhaber-zz/charters4textxd2018).\n",
    "\n",
    "##  Guiding questions\n",
    "- What educational ideologies--i.e., relatively coherent clusters of terms related to learning tasks, modes, methods, etc.--do charter schools use in their websites?\n",
    "- Which ideologies are more similar? Which are more different?\n",
    "\n",
    "## About my research \n",
    "My research categorizes charter schools' educational ideologies by creating dictionaries of coherent clusters of terms (through WEMs) and quantifying schools' emphasis on these ideologies (through count-based dictionary methods). I then use mixed regression models to connect identities with community characteristics (i.e., school district-level race and class) to document how charter schools' identities reflect segregated social environments.\n",
    "\n",
    "My project is currently cross-sectional, but plans are in the works to get longitudinal data on charter schools using the Internet Archive. I will use these data to examine survival and geographic dispersion of the different identity categories over time.\n",
    "\n",
    "You can find more code related to my research on [my main GitHub repo](https://github.com/jhaber-zz/Charter-school-identities) and [my project organization](https://github.com/URAP-charter)--the latter especially has lots of tools for web-scraping, data management, text analysis, and more.\n",
    "\n",
    "## About sharing data\n",
    "I'm happy to share and explore these data with you! And as a graduate student who spent years painstakingly collecting them, I'm not yet ready to share these data with the world. So please respect my choice to share with you, the hack session participants, by NOT downloading these data. If you're interested in continuing your analysis or in collaborating with me, please reach out to me at jhaber@berkeley.edu. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import key packages:\n",
    "import gensim # for word embedding models\n",
    "from sklearn.manifold import TSNE # For visualizing word embeddings\n",
    "\n",
    "import matplotlib # for visualizations\n",
    "import matplotlib.pyplot as plt # for easy access\n",
    "import seaborn as sns # To make matplotlib prettier\n",
    "\n",
    "# Visualization parameters\n",
    "% pylab inline \n",
    "% matplotlib inline\n",
    "matplotlib.style.use('fivethirtyeight')\n",
    "sns.set(style='white')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data and model file paths\n",
    "charters_data_path = \"../data/charter_data_201516.csv\" # statistical data on ~5K charter schools (no text)\n",
    "charters_text_path = \"../data/charter_text_201516.csv\" # statistical and text data on ~1K charter schools\n",
    "model_path = \"../data/word2vec.charters.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdrive-dl -o ../data/word2vec.charters.bin https://drive.google.com/open?id=1EX69NzGNc4HIl2gV9suXv23ivAPHChl1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & inspect model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word2vec model:\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show list of words in model\n",
    "print(len(model.vocab), \"unique words/common phrases in model vocabulary\")\n",
    "sorted(list(model.vocab)[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embedding models excel at detecting relationships between words. For instance, some of the most useful things you can do are to detect words similar to a given word, or to look for words similar to a given word group but dissimilar to some other word group.\n",
    "\n",
    "Note that this model is created using words as well as common phrases in the corpus, such that a commonly co-occurring word pair such as \"critical thinking\" is expressed as the unigram \"critical_thinking\". Other common word pairs are joined together without a space (probably due to some flaw in the extraction/download process), such that \"inquiry based\" becomes \"inquirybased\".\n",
    "\n",
    "Note also that the distances between word vectors are established via cosine similarity. Cosine similarity measures the angle between vectors such that a score of 0 indicates perfect opposition (orthogonality or 90 degrees between vectors) and 1 indicates perfect similarity (parallelity or 0 degrees between vectors). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find 10 most similar words to a single word...\n",
    "model.most_similar(\"critical_thinking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...or 15 most similar to a group of words\n",
    "model.most_similar([\"critical_thinking\", \"inquiry\", \"problem_solve\", \"experiential\"], topn=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find cosine distance between two given word vectors\n",
    "model.similarity('problem_solve','disciplinary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine contrasting word vectors\n",
    "model.most_similar(positive = [\"problem_solve\"], negative = [\"disciplinary\"], topn = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect discursive themes \n",
    "\n",
    "Here are some tools for examining whether a dictionary (group of thematically related terms) has _coherence_ (close relations between component words of a given dictionary in terms of semantics and meaning) and _distinctiveness_ (distant relations between component words of a given dictionary and words of some other, supposedly distinct dictionary). Below are also some ways to visualize these contrasts use TSNE, a probability-based method of projecting vector relationships from a high-dimensional space (here, 300) to a more easily visualized low-dimensional space (here, 2).\n",
    "\n",
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dict_cohere(thisdict, wem_model):\n",
    "    '''Computes the average cosine similarity score of terms within one dictionary with all other terms in that same dictionary,\n",
    "    effectively measuring the coherence of the dictionary.\n",
    "    ...question for development: does it make sense to compare the average cosine similarity score between all terms \n",
    "    in thisdict and the average cosine similarity among the total model vocabulary? (Could that be, by definition, 0?)\n",
    "    \n",
    "    NOTE: For an unknown reason, calling this function deletes terms from thisdict.\n",
    "    \n",
    "    Inputs: List of key terms, word2vec model.\n",
    "    Output: Average cosine similarity score of each word with all other words in the list of key terms.'''\n",
    "    \n",
    "    # Initialize average distance variables:\n",
    "    word_avg_dist = 0\n",
    "    word_avg_dists = 0\n",
    "    dict_avg_sim = 0\n",
    "    all_avg_dists = 0\n",
    "    model_avg_dists = 0\n",
    "    \n",
    "    # Compute average cosine similarity score of each word with other dict words:\n",
    "    for word in thisdict:\n",
    "        word_avg_dist = (wem_model.distances(word, other_words=thisdict).sum())/len(thisdict) # Total diffs of word with all other words, take average\n",
    "        word_avg_dists += word_avg_dist # Add up each average distance, incrementally\n",
    "    dict_avg_sim = 1 - word_avg_dists/len(thisdict) # Find average cosine similarity score by subtracting avg. distance from 1\n",
    "\n",
    "    # For comparison, compute average cosine similarity score of each word with ALL other words in the model vocabulary:\n",
    "    #for word in thisdict:\n",
    "    #    all_avg_dist = (wem_model.distances(word).sum())/len(model.vocab) # Default is to compare each word with all words\n",
    "    #    all_avg_dists += all_avg_dist\n",
    "    #model_avg_dist = 1 - all_avg_dists/len(model.vocab) # Find average cosine similarity score by subtracting avg. distance from 1\n",
    "\n",
    "    #print(\"Average cosine similarities by word for this dictionary:       \\t\" + str(dict_avg_dist))\n",
    "    #print(\"Compare to avg. cosine similarities by dict words to ALL words:\\t\" + str(model_avg_dist))\n",
    "    \n",
    "    return dict_avg_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dict_distinct(dict1, dict2, wem_model):\n",
    "    '''Computes the average cosine distance score of terms in dict1 with all terms in dict2,\n",
    "    effectively measuring the opposition/non-coherence between the two dictionaries.\n",
    "    \n",
    "    NOTE: For an unknown reason, calling this function deletes terms from thisdict.\n",
    "    \n",
    "    Inputs: List of key terms, word2vec model.\n",
    "    Output: Average cosine distance score of each word in dict1 with all words in dict2.'''\n",
    "    \n",
    "    # Initialize average distance variables:\n",
    "    word_avg_dist = 0\n",
    "    word_avg_dists = 0\n",
    "    dicts_avg_dist = 0\n",
    "    \n",
    "    # Compute average cosine similarity score of each term in dict1 with all terms in dict2:\n",
    "    for word in dict1:\n",
    "        word_avg_dist = (wem_model.distances(word, other_words=dict2).sum())/len(dict2) # Total diffs of word with all other words, take average\n",
    "        word_avg_dists += word_avg_dist # Add up each average distance, incrementally\n",
    "    dicts_avg_dist = word_avg_dists/len(dict1) # Find average cosine distance score by dividing sum by # words\n",
    "    \n",
    "    return dicts_avg_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine coherence and distinctiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary of thematic terms:\n",
    "inquiry = ['discovery', 'exploration', 'experimentation', 'laboratory', \n",
    "         'problem', 'prbl', \n",
    "         'project_based', 'project', \n",
    "         'experiential', 'experientially',\n",
    "         'inquiry', 'openended_inquiry', \n",
    "         'constructivist', 'constructivism', \n",
    "         'socratic', 'socratic_dialogue',\n",
    "         'learner_centered']\n",
    "\n",
    "# Check semantic coherence of this dictionary: 0 means not coherent, 1 means very coherent\n",
    "dict_cohere(inquiry, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(inquiry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another example\n",
    "discipline = ['discipline', 'disciplinary', \n",
    "          'no-excuses', 'zero-tolerance', 'zero_tolerance', 'zero-tolerance_policy', \n",
    "          'expel', 'expellable', 'expulsion', 'suspension', 'suspended', \n",
    "          'misbehavior', 'disobedience', 'absenteeism', 'inappropriate', \n",
    "          'perpetrator', 'crime', 'criminal', 'illegal', 'drugs', 'drug-related', \n",
    "          'violation', 'violate', 'penalty', 'punish', 'penalize', \n",
    "          'authority', 'deterrence', 'deter', \n",
    "          'behavioral_expectations']\n",
    "\n",
    "# Assess coherence\n",
    "dict_cohere(discipline, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How different are these dictionaries? \n",
    "# NOTE: The result given is cosine DISTANCE, so higher numbers mean MORE distinctiveness (and less similarity).\n",
    "dict_distinct(inquiry, discipline, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take-aways from the above:\n",
    "The inquiry dictionary has more coherence (0.62) than does the discipline dictionary (0.41)--that is, its terms are more closely related in terms of their meanings. Also, the two dictionaries are highly distinct, with a cosine distance of 0.74. In other words, based on these measures, they are capturing very different themes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_closestwords(wem_model, word, plotnumber):\n",
    "    '''Use tSNE to display a spatial map (i.e., scatterplot) of words vectors close to given word in Word2Vec model.\n",
    "    Projects each word to a 2D space (a reduction from model size) and plots the position of each word with a label.\n",
    "    Based on this blog: \n",
    "    https://medium.com/@aneesha/using-tsne-to-plot-a-subset-of-similar-words-from-word2vec-bb8eeaea6229)\n",
    "    \n",
    "    Args:\n",
    "        Word2Vec model\n",
    "        Input word\n",
    "        Number of words to plot\n",
    "    Returns:\n",
    "        Visualization of relationships between given word and its closest word vectors\n",
    "    '''\n",
    "    \n",
    "    dimensions = 300\n",
    "    \n",
    "    arr = np.empty((0,dimensions), dtype='f')\n",
    "    word_labels = [word]\n",
    "\n",
    "    # get close words\n",
    "    close_words = wem_model.similar_by_word(word, plotnumber)\n",
    "    \n",
    "    # add the vector for each of the closest words to the array\n",
    "    arr = np.append(arr, np.array([model[word]]), axis=0)\n",
    "    for wrd_score in close_words:\n",
    "        wrd_vector = model[wrd_score[0]]\n",
    "        word_labels.append(wrd_score[0])\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    x_coords = Y[:, 0]\n",
    "    y_coords = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.scatter(x_coords, y_coords)\n",
    "\n",
    "    for label, x, y in zip(word_labels, x_coords, y_coords):\n",
    "        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
    "    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)\n",
    "    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display 50 closest words to 'discipline'\n",
    "display_closestwords(model, \"expel\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 50 closest words to 'inquiry-based'\n",
    "display_closestwords(model, \"inquiry-based\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize a binary contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a combined dictionary with 100 words for each concept\n",
    "numterms = 100\n",
    "inquiry_big = inquiry + [word for (word, closeness) in model.most_similar(inquiry, topn=(numterms-len(inquiry)))]\n",
    "discipline_big = discipline + [word for (word, closeness) in model.most_similar(discipline, topn=(numterms-len(discipline)))]\n",
    "\n",
    "core_words = inquiry + discipline\n",
    "all_words = inquiry_big + discipline_big\n",
    "\n",
    "print(\"Some words from the combined dictionary: \")\n",
    "print(sorted(all_words[:30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing for visualization: find similarities between two focal terms and all words we'll map\n",
    "x = [model.similarity('inquiry-based', word) for word in all_words]\n",
    "y = [model.similarity('expel', word) for word in all_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# here's a visual of the inquiry/discipline binary: \n",
    "# top-left half is formal discipline, bottom-right half is inquiry-based learning\n",
    "# words from core dictionary are highlighted\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "ax.scatter(x, y, alpha=1, color='b')\n",
    "for i in range(len(all_words)):\n",
    "    if all_words[i] in core_words:\n",
    "        ax.annotate(all_words[i], (x[i], y[i]), fontweight='bold', color=\"blue\").set_fontsize(16)\n",
    "    else:\n",
    "        ax.annotate(all_words[i], (x[i], y[i])).set_fontsize(16)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_xlabel('Cosine similarity with word \"inquiry-based\"')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_ylabel('Cosine similarity with word \"expel\"')\n",
    "#ax.set_title()\n",
    "ax.yaxis.label.set_fontsize(24)\n",
    "ax.xaxis.label.set_fontsize(24)\n",
    "for item in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(20)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary of formal discipline (top-left) and inquiry-based learning (bottom-right) word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save visual for later\n",
    "fig.savefig(\"../data/WEM-visual_TextXD18.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
